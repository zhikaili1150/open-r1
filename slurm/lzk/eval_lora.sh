#!/bin/bash

# ç”¨æ³•:
# bash eval_lora.sh <base_model_path> <lora_paths...>

BASE_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B

# çˆ¶ç›®å½•
PARENT_DIR="data/DeepSeek-R1-Distill-Qwen-1.5B-GRPO/exp5_lr"

# æ¸…ç©ºæ•°ç»„
LORA_PATHS=()

# éåŽ† PARENT_DIR ä¸‹æ‰€æœ‰ä¸€çº§å­ç›®å½•ï¼ˆä»…ç›®å½•ï¼‰
for dir in "$PARENT_DIR"/*/ ; do
    # åŽ»æŽ‰ç»“å°¾çš„æ–œæ ï¼Œå­˜è¿›æ•°ç»„
    LORA_PATHS+=("${dir%/}")
done

printf '%s\n' "${LORA_PATHS[@]}"

source openr1/bin/activate

# éåŽ†æ¯ä¸ªLoRAè·¯å¾„
for LORA_PATH in "${LORA_PATHS[@]}"; do
    echo "=========================================="
    echo ">>> Evaluating LoRA: $LORA_PATH"
    echo "=========================================="
    
    MERGED_PATH="$LORA_PATH/merged_model"
    
    echo ">>> Merging LoRA into base model with merge_lora_model.py ..."
    mkdir -p "$MERGED_PATH"
    
    python merge_lora_model.py "$BASE_MODEL" "$LORA_PATH" "$MERGED_PATH"
    
    echo ">>> Starting evaluation with lighteval..."
    
    export VLLM_WORKER_MULTIPROC_METHOD=spawn
    
    MODEL="$MERGED_PATH"
    MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:2048,temperature:0.6,top_p:0.95}"
    OUTPUT_DIR=.
    
    TASK=gsm8k
    
    lighteval vllm $MODEL_ARGS "lighteval|$TASK|0|0" \
        --use-chat-template \
        --output-dir $OUTPUT_DIR
    
    # æ¸…ç†åˆå¹¶çš„æ¨¡åž‹
    rm -rf $MERGED_PATH
    
    echo "=========================================="
    echo ">>> Finished: $LORA_PATH"
    echo "=========================================="
    echo ""
done

echo "ðŸŽ‰ All LoRA evaluations completed!"
