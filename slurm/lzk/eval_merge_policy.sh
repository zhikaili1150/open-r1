#!/bin/bash

#SBATCH --job-name=lzk_exp4
#SBATCH --output=logs/%j_exp4_eval_merged_policy.out
#SBATCH --error=logs/%j_exp4_eval_merged_policy.err
#SBATCH --partition=h100
#SBATCH --gres=gpu:1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=32GB

source ~/.bashrc
source openr1/bin/activate

# ç”¨æ³•:
# bash eval_lora.sh <base_model_path> <lora_paths...>

BASE_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B

# çˆ¶ç›®å½•
# PARENT_DIR="data/DeepSeek-R1-Distill-Qwen-1.5B-GRPO/exp5_lr"
PARENT_DIR="data/merged_policy"

# æ¸…ç©ºæ•°ç»„
LORA_PATHS=()

# éåŽ† PARENT_DIR ä¸‹æ‰€æœ‰ä¸€çº§å­ç›®å½•ï¼ˆä»…ç›®å½•ï¼‰
for dir in "$PARENT_DIR"/*/ ; do
    # åŽ»æŽ‰ç»“å°¾çš„æ–œæ ï¼Œå­˜è¿›æ•°ç»„
    LORA_PATHS+=("${dir%/}")
done

printf '%s\n' "${LORA_PATHS[@]}"

# éåŽ†æ¯ä¸ªLoRAè·¯å¾„
for LORA_PATH in "${LORA_PATHS[@]}"; do
    echo "=========================================="
    echo ">>> Evaluating LoRA: $LORA_PATH"
    echo "=========================================="
    
    MERGED_PATH="$LORA_PATH/merged_model"
    
    echo ">>> Merging LoRA into base model with merge_lora_model.py ..."
    mkdir -p "$MERGED_PATH"
    
    python merge_lora_model.py "$BASE_MODEL" "$LORA_PATH" "$MERGED_PATH"
    
    echo ">>> Starting evaluation with lighteval..."
    
    export VLLM_WORKER_MULTIPROC_METHOD=spawn
    
    MODEL="$MERGED_PATH"
    MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:4096,temperature:0.6,top_p:0.95}"
    OUTPUT_DIR=.
    
    TASK=gsm8k
    
    lighteval vllm $MODEL_ARGS "lighteval|$TASK|8|0" \
        --use-chat-template \
        --output-dir $OUTPUT_DIR
    
    # æ¸…ç†åˆå¹¶çš„æ¨¡åž‹
    rm -rf $MERGED_PATH
    
    echo "=========================================="
    echo ">>> Finished: $LORA_PATH"
    echo "=========================================="
    echo ""
done

echo "ðŸŽ‰ All LoRA evaluations completed!"
